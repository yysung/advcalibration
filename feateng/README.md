Feature Engineering
=

Before, you built a logistic regression classifier from scratch.  Now, we're
going to *improve* a logistic regression classifier, using a pre-made system
for learning the weights from data.

In this homework, you're going to continue to improve the accuracy of such a
system by creating new features (but as we'll talk about in a moment,
"accuracy" isn't exactly the best metric to use as our primary objective).

To make things a little more complicated, you'll be running your classifier
over the output of another AI system: a model generating answers to questions.
In the lingo of the code, this system is called a "guesser", and its outputs
are guesses: the challenge is to then make the binary decision of whether this
guess is correct or not.  This decision is made by the binary classifier whose
features you are improving.

You will improve the classification by extracting useful information from
the guesses and generate better features for input into the *logistic
regression* classifier to do a better job of selecting whether a guess to a
question is correct.

In the code, a guess is generated by a "Guesser".  This is the foundation of
what you'll put into your classifier.  You need to decide whether to "buzz in"
on that guess.  This decision is made by your classifer, the "Buzzer".  In a
perfect world, every time your guess was correct, you'd buzz in and every time
your guess was wrong you would not.  The initial code here is not perfect, so
you're going to use feature engineering to improve that.

NOTE: Because the goal of this assignment is feature engineering, not
classification algorithms, you may not change the underlying algorithm. You
can change / add to the guessed answers (e.g., to create a new feature), but you
may not swap out the class that's generating classes nor can you change the
classifier.

This assignment is structured in a way that approximates how classification
works in the real world: features are typically underspecified (or not
specified at all). You have to articulate the features you
need. You then compete against others to provide useful predictions.

It may seem straightforward, but do not start this at the last minute. There
are often many things that go wrong in testing out features, and you'll want
to make sure your features work well once you've found them.

Likewise, because this homework is not going to tell you exactly what you need
to do, you'll need to have understood many of the concepts we've covered in
the class: classification obviously, but also tokens or using linguistic
concepts to generate interesting/useful features.

Getting Started
-

As usual, install the packages you need, perhaps in a virtual environment:

    python3 -m venv .venv
    .venv/bin/pip3 install -r requirements.txt

And if NLTK complains about missing stopwords, you can download them:

    python -m nltk.downloader stopwords

You'll also need to create a directory for the models you'll be
creating

     mkdir -p models

But before you get started, you need to understand the overall structure of the code:
 * A part of the question comes into the guesser (in the code, this is called a "run")
 * The guesser generates a "guess" that _could_ be an answer to the question
 * The buzzer then needs to determine if that guess is correct or not.  This is a classifier.  You're going to make that better by providing the buzzer with new features.

You will need to be creative here!  To get a sense of how you might want to go
through the process, review a lecture on feature engineering for this task here:
https://www.youtube.com/watch?v=IzKFgigocAg

While the guesses you're using are coming from OpenAI's GPT, you won't need to
use the API, we've cached all of the guesses for you.  Many students sometimes
have trouble using those cached guesses, so a good sanity check is to make
sure you have access to all of the cached guesses.

If this working correctly, you should be able to run this and see:


   > ./venv/bin/python3 gpr_guesser.py --fold=buzztrain
      ...
    Loaded 1152 question
    Generating runs of length 100
    100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1152/1152 [00:00<00:00, 35838.38it/s]
   ---------------------
    1.0
    INFO:root:Hit ratio: 1.000000
    Saving to models/buzztrain_gpr_cache
    INFO:root:Made 0 new queries, saving to models/buzztrain_gpr_cache [cf: models/buzztrain_gpr_cache / queries: False]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 441/441 [00:00<00:00, 894.84it/s]
    100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████|
    441/441 [00:00<00:00, 1036.50it/s]

If you don't see a 1.00 hit rate, something is wrong and everything else won't
work after this.  Note that you won't be able to ask it for *new* guesses that
are not in the cache, as that would require an API call.

How to add a feature?
-

First, get an idea of what you want to do.  After training the classifier,
look at your predictions on the dev set and see where they're going wrong.

1.  To add a feature, you need to create a new subclass of the Feature class
in ``features.py``.  This is important so that you can try out different
features by turning them on and off with the command line.

2.  Add code to instantiate the feature in ``parameters.py``.

3.  (Optionally) Change the API to feed in more information into the feature
generation process.  This would be necessary to capture temporal dynamics or
use, say, information from Wikipedia:
https://drive.google.com/file/d/1-AhjvqsoZ01gz7EMt5VmlCnVpsE96A5n/view?usp=sharing

To walk you through the process, let's create a new feature that encodes how
often the guess appeared in the training set.  The first step is to define the
class in ``features.py``.

    class FrequencyFeature(Feature):
       def __init__(self, name):
          from eval import normalize_answer
          self.name = name
          self.counts = Counter()
          self.normalize = normalize_answer

      def add_training(self, question_source):
          import json
          import gzip
          if 'json.gz' in question_source:
              with gzip.open(question_source) as infile:
                  questions = json.load(infile)
          else:
              with open(question_source) as infile:
                  questions = json.load(infile)
          for ii in questions:
              self.counts[self.normalize(ii["page"])] += 1

      def __call__(self, question, run, guess, guess_history, other_guesses=None):
          yield ("guess", log(1 + self.counts[self.normalize(guess)]))


Pay attention to the ``call`` function.  If you're not familiar with
the ``yield`` keyword:
https://realpython.com/introduction-to-python-generators/

One very easy way of adding features is to just yield something else
in a function that you already have.

These will be sent to a DictVectorizer, the first element of the tuple
is the feature name, the second element of the tuple is the feature
value (look at the ``featurize`` function in buzzer.py).

Now that you have a feature class, it needs to be loaded when you run
your code.  This happens in ``parameters.py``.  Now you can
add the feature name to the command line to turn it on.

    for ff in flags.features:
        if ff == "Length":
            from features import LengthFeature
            feature = LengthFeature(ff)
            buzzer.add_feature(feature)

        if ff == "Frequency":
            from features import FrequencyFeature
            feature = FrequencyFeature(ff)
            feature.add_training("../data/qanta.buzztrain.json.gz")
            buzzer.add_feature(feature)

How to Debug a Feature
========================

Okay, now we're going to make the feature that's already added there a little
bit better.  This is likely how much of what you're going to do will look
like.  To do this, you'll need to train a classifier on the feature, see
what's happening to the weights, adjust, and retrain the model.

Don't forget that you're (re)training a classifier every time you change a
feature set.  This classifier will be turned into a "pickle" file and stored
in the models directory.  So let's train the classifier *without* that new
feature, and we'll want our new set of features to be better than that.

    mkdir -p models
    ./venv/bin/python3 buzzer.py --guesser_type=gpr --limit=500 \
      --gpr_guesser_filename=../models/buzztrain_gpr_cache   \
      --questions=../data/qanta.buzztrain.json.gz --buzzer_guessers gpr \
      --logistic_buzzer_filename=models/no_length --features ""

    ... snip ...

    Initializing features: ['']
    dataset: ../data/qanta.buzztrain.json.gz
    ERROR:root:1 features on command line (['']), but only added 0 (set()).  Did you add code to params.py's load_buzzer to actually add the feature to the buzzer?  Or did you forget to increment features_added in that function?
    INFO:root:Loading questions from ../data/qanta.buzztrain.json.gz
    INFO:root:Read 500 questions
    INFO:root:Generating runs of length 100
    100%|███████████████████████████████████████████████████████| 500/500 [00:00<00:00, 150010.87it/s]
    INFO:root:Building guesses from dict_keys(['gpr'])
    INFO:root:Generating guesses for 3934 new question
    100%|█████████████████████████████████████████████████████| 3934/3934 [00:00<00:00, 386697.73it/s]
    INFO:root:      3934 guesses from gpr
    INFO:root:Generating all features
    100%|███████████████████████████████████████████████████████| 3934/3934 [00:00<00:00, 9012.54it/s]
    INFO:root:Saving buzzer to models/no_length.model.pkl
    Ran on 500 questions of 500

If you get a warning about convergence, it is okay; hopefully it will converge
better with more features!  Likewise, don't worry about the warning about the
features, I just wanted to be sure it didn't add the length feature.  Because
we want to do that next: train a model *with* that new feature.  Note that
we're naming the model something different:

    ./venv/bin/python3 buzzer.py --guesser_type=gpr --limit=500 --gpr_guesser_filename=../models/buzztrain_gpr_cache   --questions=../data/qanta.buzztrain.json.gz --buzzer_guessers gpr --logistic_buzzer_filename=models/with_length --features Length

    ... snip ...

    Initializing features: ['Length']
    dataset: ../data/qanta.buzztrain.json.gz
    INFO:root:Adding feature Length
    INFO:root:Loading questions from ../data/qanta.buzztrain.json.gz
    INFO:root:Read 500 questions
    INFO:root:Generating runs of length 100
    100%|███████████████████████████████████████████████████████| 500/500 [00:00<00:00, 153323.00it/s]
    INFO:root:Building guesses from dict_keys(['gpr'])
    INFO:root:Generating guesses for 3934 new question
    100%|█████████████████████████████████████████████████████| 3934/3934 [00:00<00:00, 394180.41it/s]
    INFO:root:      3934 guesses from gpr
    INFO:root:Generating all features
    100%|███████████████████████████████████████████████████████| 3934/3934 [00:00<00:00, 8675.89it/s]
    INFO:root:Saving buzzer to models/with_length.model.pkl
    Ran on 500 questions of 500

Now you need to evaluate the classifier.  The script eval.py will run the
classifier on all of your data and then report the outcome.

Let's compare with the Length:

    .venv/bin/python3  eval.py --guesser_type=gpr   --limit=25   \
    --questions=../data/qanta.buzzdev.json.gz --buzzer_guessers gpr \
	--gpr_guesser_filename=../models/buzzdev_gpr_cache    \
	--logistic_buzzer_filename=models/with_length --features Length

compared to without it:

    .venv/bin/python3  eval.py --guesser_type=gpr   --limit=25   \
    --questions=../data/qanta.buzzdev.json.gz --buzzer_guessers gpr \
	--gpr_guesser_filename=../models/buzzdev_gpr_cache    \
	--logistic_buzzer_filename=models/no_length --features ""

You'll see quite a bit of output, so we're just going to walk through it
bit by bit, comparing the salient components.

Initially with and without the length feature will not look that different.

With Length:

    .venv/bin/python3  eval.py --guesser_type=gpr   --limit=25   --questions=../data/qanta.buzzdev.json.gz --buzzer_guessers gpr --gpr_guesser_filename=../models/buzzdev_gpr_cache    --logistic_buzzer_filename=models/with_length --features Length
                            Length_guess: -1.0023
                          gpr_confidence: 7.6883
    Questions Right: 84 (out of 201) Accuracy: 0.82  Buzz ratio: 0.36 Buzz position: -0.135464

Without length:

    .venv/bin/python3  eval.py --guesser_type=gpr   --limit=25   --questions=../data/qanta.buzzdev.json.gz --buzzer_guessers gpr --gpr_guesser_filename=../models/buzzdev_gpr_cache    --logistic_buzzer_filename=models/no_length --features ""
                          gpr_confidence: 7.5694
    Questions Right: 82 (out of 201) Accuracy: 0.80  Buzz ratio: 0.34 Buzz position: -0.095168

So let's take a look at that code.

    class LengthFeature(Feature):
      """
      Feature that computes how long the inputs and outputs of the QA system are.
      """

      def __call__(self, question, run, guess, guess_history, other_guesses=None):
        # How many characters long is the question?

        guess_length = 0
        guess_length = log(1 + len(guess))

        # How many words long is the question?


        # How many characters long is the guess?
        if guess is None or guess=="":
            yield ("guess", -1)
        else:
            yield ("guess", guess_length)

Well, it's just using the length of the guess and not the length of the
question "run", i.e., the question that's being asked.  That doesn't seem very
good, right?  So let's add in another feature that keeps track of how many
characters long the run is.

    class LengthFeature(Feature):
      """
      Feature that computes how long the inputs and outputs of the QA system are.
      """

      def __call__(self, question, run, guess, guess_history, other_guesses=None):
        # How many characters long is the question?

        guess_length = 0
        guess_length = log(1 + len(guess))
        yield ("guess", guess_length)

        yield ("char", log(1 + len(run)))

Don't forget to rerun your training of the buzzer!  Now we have some much better results:

                             Length_char: 0.7869
                            Length_guess: -1.0144
                          gpr_confidence: 6.8528
    Questions Right: 84 (out of 201) Accuracy: 0.84  Buzz ratio: 0.36 Buzz position: 0.142734


We can now take a look at *examples* to see where we are still having issues.    There are several
things that could happen:

 * _best_: Guess was correct, Buzz was correct
 * _timid_: Guess was correct, Buzz was not
 * _aggressive_: Guess was wrong, Buzz was wrong
 * _waiting_: Guess was wrong, Buzz was correct


Now, both "best" and "waiting" are *correct*, but obviously "best" is best.
It's important to know what kind of examples contribute to each of these
outcomes, so eval samples a subset for each of these and prints them and their
features out.


    =================
    aggressive 0.11
    ===================

               guess: The Awakening (Chopin novel)
              answer: Edna_Pontellier
                  id: 93160
      gpr_confidence: -0.0008
        Length_guess: 3.3673
         Length_char: 6.4036
                text: This character faintheartedly commits herself to improving her studies
                      after a night of reading Emerson alone in her house, and hushes Victor
                      when he begins singing "Ah! Si tu savais!" While talking to a friend,
                      she declares that she would give up the "unessential things" for her
                      children, but she wouldn't give herself up. Doctor Mandelet advises
                      this character's husband to permit her whims, which include moving
                      into a "pigeon house" outside of her house on Esplanade Street. This
                      mother of Raoul and Etienne watches Adele Ratignolle give birth on her
                      last night alive, and romances Alcee Arobin and

This example is where it is answering the name of the novel rather than the book's main character.  You can see all of the features for this example (e.g., Length_char is 6.4).

At the end of the eval script, you can see the
overall accuracy, and the ratio of correct buzzes to incorrect buzzes (should
be positive), and the buzz position (where in the question it's buzzing).

Don't focus too much on the accuracy.  The proportion of "Best" outcomes is a better measure of how well you're doing.

At the very end of the script, you see the weights of each of the features.
Higher values mean that when the feature is high, it is more likely to buzz.
Lower features mean that when the feature is high, it is less likely to buzz.
Features near zero are ignored.  However, keep in mind the average value of
the feature ... I'd encourage you to keep your features with mean zero and
standard variance to make your life easier.

Let's see with length:
                             Length_char: 0.7869
                            Length_guess: -1.0144
                          gpr_confidence: 6.8528
And without length:
                          Gpr_confidence: 5.5703

The classifier with the length is more liketly to buzz later in the question.
If you only have the guesser confidence, then it's obviously correlated with
that.  It uses it less if you add in the length as a feature.  But as the
guess gets longer, it's correlated with less buzzing.

What Can You Do?
-

You can:
* Add features (e.g., to parameters.py)
* Change feature representations (e.g., features.py)
* Exclude data
* Add data

Good Enough
-

This is a very open-ended assignment.  Improve the "best" class by at least
0.02 percent or improve the buzz ratio by 0.02 by adding new features, and you
have done enough.

What Can't You Do?
-
Change the static guesses or use a different classifier (buzzer in this lingo).

How to start
-
1. Remind yourself how to run the sklearn logistic regression (logistic_buzzer.py)
2. Add a simple feature to the training data generated by gpr_guesser.py
3. See if it increases the accuracy on held-out data when you run logistic regression (eval.py) or on the leaderboard
4. Rinse and repeat!


Finding Correct Guesses (15+ points)
------------------------------

15 points of your score will be generated from your performance on the
the classification competition on the leaderboard.  The performance will be
evaluated on a held-out test set.  As discussed in more detail in the "good enough" section, we mostly care about increasing the proportion of "best" outcomes and improving the buzz ratio, and raw accuracy alone can be misleading.

You should be able to significantly
improve on the baseline system.  If you can
do much better than your peers, you can earn extra credit (up to 10 points).

Analysis (10 Points)
--------------

The job of the written portion of the homework is to convince the grader that:
* Your new features work
* You understand what the new features are doing
* You had a clear methodology for incorporating the new features

Make sure that you have examples and quantitative evidence that your
features are working well, and include the metrics you chose to measure your system's performance. Be sure to explain how used the data
(e.g., did you have a development set?) and how you inspected the
results.

A sure way of getting a low grade is simply listing what you tried and
reporting the corresponding metrics for each attempt.  You are expected to pay more
attention to what is going on with the data and take a data-driven
approach to feature engineering.

How to Turn in Your System
-
* ``features.py``: This file includes an implementation of your new features.
* ``parameters.py``: This instantiates your new features.  Modify this so that the
set of your best features runs by *default*.  In other words, change
this line so that instead of the empty list, it loads your favorite features:

     parser.add_argument('--features', nargs='+', help='Features to feed into Buzzer', type=str,  default=[])

* **Custom Training Data** (If you used additional training data beyond the Wikipedia pages, upload that as well
    * (OR) If either any of your files are >100MB, please submit a shell
    script named ``gather_resources.sh`` that will retrieve one or both of the
    files above programatically from a public location (i.e a public S3
    bucket).
* The LogisticBuzzer.model.pkl file and LogisticBuzzer.featurizer.pkl file created by training the classifier.
* ``analysis.pdf``: Your **PDF** file containing your feature engineering
analysis.

Turn in the above files as usual via Gradescope, where we'll be using the
leaderboard as before.  However, the position on the leaderboard will count
for more of your grade.

FAQ
-----------------
**Q.: How can I improve the "waiting" category.**

**A.:** That's the neat thing, you don't.  If the guesser is wrong, then there's nothing you can do to make it correct (future homeworks won't have that problem).  What you can do is to convert "timid" to "best" and convert "aggressive" to "waiting".  

**Q. Eval only shows me what the questions I'm getting right and wrong
are.  How do I know what the features look like?**

**A.** Use ``features.py`` to investigate this.  This is how we
generated the JSON files for the logistic regression homework.

    ./venv/bin/python3 features.py \
	--json_guess_output=../data/inspect.jsonl --buzzer_guessers 'gpr' \
	--questions=../data/qanta.buzzdev.json.gz --limit=1000 \
	--guesser_type=gpr \
	--gpr_guesser_filename=../models/buzzdev_gpr_cache \
	--features Length Frequency

Make sure that you've enabled all of the features that you want to
use, and you can see how the examples look like to the classifier by
inspecting `../data/inspect.json`.

**Q. Why can't I use ``['page']`` or ``['answer']`` when creating
features?  Can I use it during training?**

**A.** Remember that we have multiple folds of the data, and we're using mostly buzztrain
and buzzdev in this homework.  For those fields you cannot use "page" / "text" when
generating features for the example you're trying to decide whether or not to trust the guess,
as that would be cheating.  That's why they get removed
before the feature generator is called (in ``add_data`` in ``buzzer.py``) so that you cannot cheat.  If you need the current text
available, that's the "run", and your job is to see if the current
"guess" is correct or not.

Now, that's not to say that you can never use the page field.  You can use the field
from *other* questions to better understand the distribution of answers, questions, etc.  You can see this
in the example Frequency feature: it uses the page to compute how
often each correct response is in the guesstrain fold.  You then check for a *guess* that comes
in how frequent it is in guesstrain; the intuition is that if the guess is more frequent,
it might be more likely to be an answer.  Or if something has never been an answer before,
maybe you should not risk it (and perhaps the relationship is non-linear, and you might need
to add cutoffs or thresholds ... hint, hint).

But that's the exception, usually the only way you would use the real
'page' during training on the buzzdev fold is as the label to the classifier: is this
guess correct becomes a positive example, is this guess incorrect
becomes a negative example.

**Q: Is there a limit to how many features I can yield per class?**

Not that I know of.  Students have implemented some with thousands of features (not always a good idea, but it's possible).  You may want to have more granular classes of features to be able to more systematically turn them on and off.

For example, you might want to have a script that tries out combinations of features, runs ``eval.py``, and then plots the results.  If everything is in one class, that kind of analysis is harder to do.

**Q: Is the Length feature complete?  How can I get the length of the
  question being asked so far?**

**A:** When a feature is constructed, you have access to the the
  ``guess`` string.  From this, you can ask the number of characters,
  the number of words (e.g., by splitting it).  The provided code:

    def __call__(self, question, run, guess, guess_history, other_guesses=None):
        # How many characters long is the question?

        guess_length = 0
        guess_length = log(1 + len(guess))

        # How many words long is the question?


        # How many characters long is the guess?
        if guess is None or guess=="":
            yield ("guess", -1)
        else:
            yield ("guess", guess_length)

Only gives you the raw length in the number of characters.  You may
want to improve this feature by (for example) subtracting the mean and
dividing by the standard deviation.

**Q: Can I modify buzzer.py so that I can use the history of guesses in a
 question?**

**A:** Yes.  If you do that, make sure to upload buzzer.py.  We will replace the
 default version of buzzer.py with your new submission.

**Q: Can I use the <INSERT NAME HERE> package?**

**A:** Clear it first on Piazza.  We'll provide spacy and nltk for sure (along
 with all of the packages already used in this homework).  We
 won't allow packages that require internet access (e.g., wikipedia).  We
 don't have anything against Wikipedia (we provide this json file so you can
 use it), but we don't want to get our IP
 address banned.

**Q: Sometimes the guess is correct but it isn't counted that way.  And
 sometimes a wrong answer is counted as correct.**

**A:** Yes, and we'll cover this in more detail later in the course.  For now,
 this is something we'll have to live with.

**Q: What is the guesser that we're using?  Where are the guesses
coming from?**

**A:** These are cached guesses from OpenAI's GPT.  We'll get into
 generating our own guesses in the next homework.  You probably will
 want to play around a little bit with the output of its guesses, as
 there's likely interesting stuff you can use from there. Let's play
 around with the GprGuesser a little:


    >>> list(gg.cache.keys())[:3]
    ['The men in this work speak "of planting and rain, tractors and
    taxes". Some neighbors are denounced as a "pack of crazy old fools" to
    Mr. and Mrs. Adams by Old Man Warner. Earlier, Bobby Martin and Dickie
    Dell-a-croy are spotted playing, and some wooden chips were previously
    housed in Mr. Graves\' barn before being replaced by paper, as per the
    orders to the local coal entrepreneur. That character, Mr. Summers,
    later brings out a three-legged stool on June 27th and puts a black
    box on the stool. Townspeople like the Dunbars are relieved that they
    don\'t meet the fate of Tessie Hutchinson, who draws a slip of paper
    with a black spot on it. Name this short story in which Tessie is
    stoned to death as a result of the titular ritual, a work by Shirley
    Jackson.', 'One of the men depicted would die ten years later on the
    Gila River Indian Reservation after drinking', 'Men involved in this
    battle asked each other who Mickie Mouse\'s girlfriend was after the
    aggressors of this battle infiltrated enemy ranks and sabotaged
    progress in Operation Greif. In this battle, the Peipers, after
    capturing an enemy unit of 150 men, murdered 84 of them in an event
    known as the (*) Malmedy Massacre. The instigators of this offensive
    met fierce resistance in the city of Bastogne from a small force led
    by Brigadier General McAuliffe, who, when asked to surrender, cried
    "Nuts!" before being relieved by General Patton\'s Third Army. For ten
    points, name this last German offensive in the Ardennes, in which
    Adolf Hitler attempted to recapture Antwerp.']
    >>> question = list(gg.cache.keys())[0]
    >>> question
    'The men in this work speak "of planting and rain, tractors and taxes". Some neighbors are denounced as a "pack of crazy old fools" to Mr. and Mrs. Adams by Old Man Warner. Earlier, Bobby Martin and Dickie Dell-a-croy are spotted playing, and some wooden chips were previously housed in Mr. Graves\' barn before being replaced by paper, as per the orders to the local coal entrepreneur. That character, Mr. Summers, later brings out a three-legged stool on June 27th and puts a black box on the stool. Townspeople like the Dunbars are relieved that they don\'t meet the fate of Tessie Hutchinson, who draws a slip of paper with a black spot on it. Name this short story in which Tessie is stoned to death as a result of the titular ritual, a work by Shirley Jackson.'
    >>> gg(question)
    [{'guess': 'The Lottery', 'confidence': -0.0060731087}]

The keys are the prompts to GPT.  We've given GPT the closest examples
we can find in Wikipedia and from our existing dataset.

Let's take a look at what this returned.  We get the title, but that's
not all.  Remember that GPT is just a language model, so it generates
one word piece at a time, and we have a probability for each word
piece.

    >>> gg.cache[question]
    {'guess': 'The Lottery', 'confidence': [['The', -0.010978376], [' Lottery', -0.0011678414]]}

You may want to use these to create features beyoned the default of artithmetic average of the log probabilities!

**Q: What if I get the error that ``GprGuesser`` has no attribute 'predict'?**

**A:** This means that you're running it on a guesser result that hasn't been
 cached or that it can't find the cache file.  Make sure the path is correct,
 and use the limit option to only process a handful of examples.

 **Q: What's the intuition behind "buzz ratio"?**

**A:** It corresponds to how many points you get per question.  In the trivia community this is [points per tossup heard](https://www.naqt.com/stats/explanation.jsp#:~:text=For%20teams%2C%20PPTUH%20is%20the,an%20average%2020%2Dtossup%20game.).

**Q: Why do stupid features sometimes work?**

**A:** Features that sound stupid sometimes reveal something deeper.  For example, the length of the guess can be a proxy for GPT errors (e.g., it failed to complete its answer or didn't stop spewing content).

**Q: Why do clever features sometimes not work?**

**A:** Clever features might fail for several reasons: it's too infrequent, it's covered by another feature, it's not correlated with errors, or the feature value is not specified correctly.

_Infrequent_: All of this is a numbers game.  If your problem only appears in 1 out of 100 examples, then the gradients won't be large enough to move the feature value, which means that the feature won't have a big effect on overall predictions.

_Covered by Another Feature_: Remember that features only update when you have an error gradient.  If another feature captures the same phenomena perfectly ("correlated" if you remember this from 320), then the feature won't end up getting used.  And whatever feature is more frequent and covers more cases will get used ... all other features will be ignored.

_Not correlated with errors_: Like the above, if a system doesn't make a certain type of error, then a feature targeting that error, no matter how great, will not get used or be useful.  For example, if you create a feature that checks if a guess is consistent with a particular category won't be useful if the underlying guesser doesn't make cross-category errors.

_Not specified correctly_: One of the reasons that simple features that count stuff work well is that they are linear, one of the key assumptions of logistic regression.  If a feature value of 0.2 correlates with a good outcome, 1.1 correlates with bad outcomes, but 2.8 correlates with good outcomes again, then it's not going to be a good feature because it can't actually get encoded by a linear classifier.  You can address this by inspecting the distribution and creating threshold functions.

**Q: I'm stuck... I can't think of any features! Help!** 

**A:**  If you feel stuck, my usual advice is to take a look at examples (via the eval script) that you're getting wrong.  Do you see any patterns?  Are those patterns things that you could turn into a feature that is either 1 when they happen or a real number that correlates with how much you're seeing that pattern?  If so, code that up!  If it appears only when you're wrong, then that's something that the buzzer can use to get answers right.

And "wrong" could be buzzing when it shouldn't or not buzzing when it should, so if you see patterns that correlate with correct guesses that aren't being captured, that's also something you can add.

Sometimes people stare at the numbers too long an forget the data.
